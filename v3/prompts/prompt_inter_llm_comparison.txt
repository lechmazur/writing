You are an evaluator. Compare two short stories that were written to the SAME required elements. Your goal is to analyze which writer handles execution better or worse across:
1) rubric‑aligned axes used for grading (Q1–Q8 major aspects + Q9A–J element integration), and
2) additional interesting insights that the rubric does not explicitly cover.

Context: The required elements define the shared constraints. Judge execution quality and creative realization of those elements (not mere presence of the element words). Evaluate clarity, specificity, narrative pressure, cause/effect, emotional throughline, and closure with a felt or conceptual cost. For rubric‑aligned findings, calibrate to these categories:
- Q1–Q8 (major aspects): character, plot/closure under drive lens, setting/atmosphere, escalation/pressure, originality, thematic cohesion, voice/POV, prose/line craft.
- 9A–9J (element integration): how well the specified elements are integrated (not merely present).

Rubric at a glance (for rubric‑aligned sections)
<<<RUBRIC_SUMMARY>>>

REQUIRED ELEMENTS (shared)
<<<ELEMENTS>>>

STORY A
<<<STORY_A>>>

STORY B
<<<STORY_B>>>

Evaluation focus
- Rubric‑aligned: Compare along Q1–Q8 craft axes and 9A–J element integration (integration quality, clarity, pressure/causality, stakes, specificity, voice, line craft, and closure under an inferred drive lens). Keep observations tied to what is observable on the page.
- Beyond‑rubric insights: Surface meaningful differences not directly graded. Consider: risk appetite/ambition; humor/comedic timing (setup, restraint, callbacks); cultural specificity and concreteness; metaphor and image layering (freshness, recurrence, transformation); rhythmic or syntactic identity (cadence, sentence‑shape variety, isochrony/volta moments); idiomatic freshness and cliché avoidance; world‑knowledge salience and plausibility without over‑explaining; compression vs. scaffolding balances; sentiment calibration and tonal control (avoiding sentimentality); moral/epistemic nuance; genre fluency and pattern teaching. Tie every claim to observable signals even if the axis is not explicitly graded.
- Prefer concrete, observable signals over abstract claims. Quote at most 12 words per quote, and only if necessary. Normalize for length. Do NOT penalize valid multi‑POV/mosaic if clear and closure lands.

Output format (exact tags; keep content concise and specific)
- Compliance: ALL sections below are mandatory. Output nothing outside these tags. If a section would otherwise be empty, include a single short bullet like "- None".
- Use bullets with short lines. Avoid boilerplate. Do not include any other text outside these tags.

# Rubric‑aligned findings (Q1–Q8 + 9A–J)
<a_advantages>
- 4–7 bullets naming what A does better than B on rubric‑aligned axes (integration, clarity, stakes/pressure, closure under lens, imagery specificity, scene construction, pacing, voice/POV, line craft).
</a_advantages>
<b_advantages>
- 4–7 bullets naming what B does better than A on those same axes.
</b_advantages>
<shared_or_parity>
- 2–4 bullets for areas where execution feels comparable or tied.
</shared_or_parity>
<failure_modes>
- Up to 4 bullets calling out failure patterns that recur (specify which story exhibits each pattern more).
</failure_modes>
<verdict>
One sentence: which story handles rubric‑aligned criteria better overall and why (name 1–2 decisive factors).
</verdict>

# Beyond‑rubric insights (interesting, not directly graded)
<extra_insights>
- 5–10 bullets with sharp, falsifiable observations outside the rubric (see Evaluation focus for eligible axes). For each bullet, prefix with "A:", "B:", or "Shared:" and keep the on‑page evidence minimal but clear (no more than one short quoted fragment if needed).
- Avoid speculation about training data, safety, bias, or intentions. Stay on‑page. Prefer pattern‑level observations (recurring behaviors across the story) over one‑off anecdotes.
</extra_insights>

<flags>format_error=0</flags>
